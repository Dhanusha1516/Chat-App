{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanusha1516/Chat-App/blob/main/On_Time_Delivery_Using_Supplier_Data_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGrNgFIwVuIU",
        "outputId": "2613f859-d92c-4586-f8fb-810417353983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR: File not found. Please check the file path and name.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('Train.csv')\n",
        "    print(\"Data loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: File not found. Please check the file path and name.\")\n",
        "    df = None\n",
        "\n",
        "if df is not None:\n",
        "    print(\"\\nInitial Data Structure:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nData Shape (Rows, Columns):\", df.shape)\n",
        "    print(\"\\nData Types:\")\n",
        "    print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgzaGaPyasOA"
      },
      "source": [
        "**Load Data Set**\n",
        "Train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "e08c498b",
        "outputId": "f5626ec9-ba23-4af8-faf8-b1e223523ffc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fa5ffb60-75e6-46ca-84f0-bc2299d2cc14\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fa5ffb60-75e6-46ca-84f0-bc2299d2cc14\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3035168958.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b935228b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('Train.csv')\n",
        "    print(\"Data loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: File not found. Please check the file path and name.\")\n",
        "    df = None\n",
        "\n",
        "if df is not None:\n",
        "    print(\"\\nInitial Data Structure:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nData Shape (Rows, Columns):\", df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNSxF7ePYWdG"
      },
      "source": [
        "**Initial Data Types**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ucm79C8cYBtk"
      },
      "outputs": [],
      "source": [
        "# Initial Data Types\n",
        "print(\"Initial Data Types\")\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8h2eNahaVa7"
      },
      "source": [
        "**correct Data Types**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xIEJ956rYluW"
      },
      "outputs": [],
      "source": [
        "if 'ID' in df.columns and df['ID'].dtype in ['int64', 'float64']:\n",
        "    df['ID'] = df['ID'].astype(str)\n",
        "    print(\"ACTION: Converted 'ID' to string (object) type.\")\n",
        "\n",
        "\n",
        "numeric_cols_to_check = ['Customer_care_calls', 'Customer_rating', 'Cost_of_the_Product', 'Prior_purchases', 'Discount_offered', 'Weight_in_gms']\n",
        "\n",
        "for col in numeric_cols_to_check:\n",
        "    if col in df.columns:\n",
        "        # Convert to Int64 (capital 'I') for Pandas integer type\n",
        "        df[col] = df[col].astype('Int64')\n",
        "        print(f\"ACTION: Confirmed/Converted '{col}' to Int64.\")\n",
        "\n",
        "# The target variable 'Reached.on.Time_Y.N' (0 or 1) should be integer.\n",
        "if 'Reached.on.Time_Y.N' in df.columns:\n",
        "    df['Reached.on.Time_Y.N'] = df['Reached.on.Time_Y.N'].astype('Int64')\n",
        "    print(\"ACTION: Confirmed/Converted 'Reached.on.Time_Y.N' to Int64.\")\n",
        "\n",
        "print(\"\\ncorrected Data Types\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHfdA8NUZdY8"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU3cJzMTcCHW"
      },
      "source": [
        "**Handling Missing Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XfQFKuT3bbww"
      },
      "outputs": [],
      "source": [
        "print(\"Missing Values Report\")\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "missing_table = pd.DataFrame({'Missing Count': missing_data, 'Percentage (%)': missing_percentage})\n",
        "missing_table = missing_table[missing_table['Missing Count'] != 0].sort_values(by='Missing Count', ascending=False)\n",
        "print(missing_table)\n",
        "\n",
        "\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['int64', 'float64']:\n",
        "        # Fill numeric NaNs with the median\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "    elif df[col].dtype == 'object':\n",
        "        # Fill categorical/object NaNs with 'Unknown'\n",
        "        df[col].fillna('Unknown', inplace=True)\n",
        "    # Re-check to confirm missing values are handled\n",
        "    # print(f\"Missing values in {col} after cleaning: {df[col].isnull().sum()}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpnLO0-KcM_K"
      },
      "source": [
        "**Remove Duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PNZWMAiRbl__"
      },
      "outputs": [],
      "source": [
        "# Remove Duplicates\n",
        "print(\"\\nDuplicates\")\n",
        "initial_rows = len(df)\n",
        "df.drop_duplicates(inplace=True)\n",
        "duplicates_removed = initial_rows - len(df)\n",
        "print(f\"Total Duplicate Rows Found and Removed: {duplicates_removed}\")\n",
        "print(f\"Current Data Shape after removing duplicates: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja987OggcfOR"
      },
      "source": [
        "**Counts missing values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8TFcqn0nb65a"
      },
      "outputs": [],
      "source": [
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nTotal missing values:\", df.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eHfTfa-c6ti"
      },
      "source": [
        "**Summarizes data distribution**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPUOe6t6dSyF"
      },
      "source": [
        "Numerical Descriptive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5TSC_3RDZLZv"
      },
      "outputs": [],
      "source": [
        "print(\"\\nNumerical Descriptive\")\n",
        "# Generates count, mean, std, min, max, and quartiles for numeric columns\n",
        "print(df.describe().T)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Categorical Features\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# List of all categorical/object columns\n",
        "categorical_cols = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        print(f\"\\nDistribution for: {col}\")\n",
        "\n",
        "        value_counts = df[col].value_counts()\n",
        "        percentage = df[col].value_counts(normalize=True) * 100\n",
        "\n",
        "\n",
        "        distribution_table = pd.DataFrame({'Count': value_counts, 'Percentage (%)': percentage.round(2)})\n",
        "        print(distribution_table.head(5))\n",
        "        print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q4VYZ_JdhW2"
      },
      "source": [
        "Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qmqLwIBwdiE8"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCategorical Features\")\n",
        "categorical_cols = df.select_dtypes(include='object').columns\n",
        "\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nValue Counts for: {col}\")\n",
        "\n",
        "    print(df[col].value_counts(normalize=True).head(10) * 100)\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkpp6c24OV55"
      },
      "source": [
        "Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SDi-vsWsOVEm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure df is defined. This block makes the cell more self-sufficient.\n",
        "if 'df' not in locals() or df is None:\n",
        "    try:\n",
        "        df = pd.read_csv('Train.csv')\n",
        "        print(\"DataFrame 'df' loaded successfully within visualization cell.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Train.csv not found. Please upload the file and ensure the cell defining df is executed.\")\n",
        "        df = None # Ensure df is None if file not found\n",
        "\n",
        "if df is not None:\n",
        "    sns.set_style(\"whitegrid\")\n",
        "\n",
        "    # a. Histograms (Distribution of Numerical Features)\n",
        "    print(\"\\nHistograms (Distribution)\")\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "    # Determine grid size for subplots for histograms\n",
        "    num_cols_to_plot_hist = len(numeric_cols)\n",
        "    num_rows_hist = (num_cols_to_plot_hist + 2) // 3  # Aim for 3 columns per row\n",
        "    num_cols_hist = 3\n",
        "\n",
        "    plt.figure(figsize=(num_cols_hist * 5, num_rows_hist * 4))\n",
        "\n",
        "    for i, col in enumerate(numeric_cols):\n",
        "        plt.subplot(num_rows_hist, num_cols_hist, i + 1)\n",
        "        plt.hist(df[col], bins=30, color='purple', edgecolor='black') # Set color to purple\n",
        "        plt.title(f'Distribution of {col}', fontsize=12)\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "    plt.suptitle('Histograms of Numerical Features', y=1.02, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # b. Boxplots (Outliers Detection)\n",
        "    print(\"\\nBoxplots (Outliers Detection)\")\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    # Re-evaluate numeric_cols in case df was just loaded\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "    num_cols_to_plot_box = len(numeric_cols)\n",
        "    num_rows_box = (num_cols_to_plot_box + 1) // 2 # Aim for 2 columns per row\n",
        "    for i, col in enumerate(numeric_cols):\n",
        "        plt.subplot(num_rows_box, 2, i + 1)\n",
        "        sns.boxplot(y=df[col], color='darkviolet') # Changed color to darkviolet\n",
        "        plt.title(f'Boxplot of {col}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # c. Scatter plots (Relationships) - Example: Cost vs. Weight\n",
        "    print(\"\\nScatter Plots (Relationships)\")\n",
        "\n",
        "    # Corrected column names based on df.head() and df.info()\n",
        "    if 'Cost_of_the_Product' in df.columns and 'Weight_in_gms' in df.columns:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.scatterplot(x='Weight_in_gms', y='Cost_of_the_Product', data=df, color='darkviolet') # Changed color to darkviolet\n",
        "        plt.title('Cost of Product vs. Weight')\n",
        "        plt.xlabel('Weight (gms)')\n",
        "        plt.ylabel('Cost of the Product')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Skipping scatter plot: 'Cost_of_the_Product' or 'Weight_in_gms' column not found in DataFrame.\")\n",
        "\n",
        "\n",
        "    # d. Correlation Heatmaps\n",
        "    print(\"\\nCorrelation Heatmap\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "    plt.title('Correlation Heatmap of Numerical Features')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Cannot proceed with visualizations as DataFrame 'df' is not loaded or available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OtR4NsBLTmJH"
      },
      "outputs": [],
      "source": [
        "# Final Correlation Analysis with Target Variable (from Step 4d data)\n",
        "\n",
        "print(\"\\n--- 5a. Correlation with Target Variable (Reached.on.Time_Y.N) ---\")\n",
        "\n",
        "try:\n",
        "    all_numerical_cols = ['Customer_care_calls', 'Customer_rating', 'Cost_of_the_Product',\n",
        "                          'Prior_purchases', 'Discount_offered', 'Weight_in_gms', 'Reached.on.Time_Y.N']\n",
        "    correlation_matrix = df[all_numerical_cols].corr()\n",
        "\n",
        "    target_corr = correlation_matrix['Reached.on.Time_Y.N'].sort_values(ascending=False)\n",
        "    print(target_corr)\n",
        "except NameError:\n",
        "    print(\"ERROR: Correlation matrix not found. Please re-run Step 4d.\")\n",
        "\n",
        "# 5b. Group by analysis: On-Time Delivery Status by Key Categorical Features\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- 5b. Group By Analysis: On-Time Performance (1) by Category ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "categorical_cols_for_group = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']\n",
        "\n",
        "for col in categorical_cols_for_group:\n",
        "    # Calculate the mean of 'Reached.on.Time_Y.N' (which is the percentage of On-Time=1)\n",
        "    # The mean of a binary column (0/1) gives the proportion of '1's\n",
        "    performance = df.groupby(col)['Reached.on.Time_Y.N'].mean().sort_values(ascending=False)\n",
        "\n",
        "    print(f\"\\nAverage On-Time Rate (Performance) by {col}:\")\n",
        "    print(performance.apply(lambda x: f\"{x*100:.2f}%\")) # Display as a percentage\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Optional: Visualization of Group-by\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    performance.plot(kind='bar', color='darkviolet') # Changed color to darkviolet\n",
        "    plt.title(f'On-Time Rate by {col}')\n",
        "    plt.ylabel('On-Time Percentage (Mean of Reached.on.Time_Y.N)')\n",
        "    plt.xticks(rotation=0 if col == 'Gender' else 45)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SRBzBgSWSeI"
      },
      "source": [
        "1. Data Loading & Initial Cleaning\n",
        "2. Feature and Target Definition\n",
        "3. Preprocessing Pipeline Construction\n",
        "    * Numerical Features\n",
        "    * Categorical Features\n",
        "    * Column Transforme\n",
        "4. Data Splitting and Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uieTvDNNVyDa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "# NOTE: Replace 'your_data_file.csv' with the actual file name if it differs\n",
        "try:\n",
        "    # Assuming the data is loaded with the cleaned column names (e.g., Target is Reached.on.Time_Y.N)\n",
        "    df = pd.read_csv('Train.csv')\n",
        "\n",
        "    # We will drop the 'ID' column as it is just an identifier and not a feature.\n",
        "    # We also drop Customer_rating as EDA showed it was a poor predictor.\n",
        "    df = df.drop(columns=['ID'])\n",
        "\n",
        "    print(\"Data loaded successfully.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The data file 'your_data_file.csv' was not found. Please check the file path.\")\n",
        "    # Create a mock DataFrame for demonstration if file is missing (to allow the code to run)\n",
        "    data = {\n",
        "        'Warehouse_block': ['A', 'B', 'C', 'A', None],\n",
        "        'Mode_of_Shipment': ['Ship', 'Road', 'Ship', 'Flight', 'Road'],\n",
        "        'Customer_care_calls': [4, 5, 2, 6, 3],\n",
        "        'Cost_of_the_Product': [150, 200, 120, 300, 180],\n",
        "        'Prior_purchases': [2, 5, 1, 8, 3],\n",
        "        'Product_importance': ['low', 'medium', 'high', 'low', 'medium'],\n",
        "        'Gender': ['F', 'M', 'F', 'M', 'F'],\n",
        "        'Discount_offered': [10, 5, 25, 2, 15],\n",
        "        'Weight_in_gms': [1200, 4500, 1500, 5500, 2800],\n",
        "        'Reached.on.Time_Y.N': [1, 0, 1, 0, 1]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    # Introducing a null value for demonstration\n",
        "    df.loc[4, 'Warehouse_block'] = np.nan\n",
        "    print(\"Loaded mock data with an intentional null value for demonstration.\")\n",
        "\n",
        "\n",
        "# --- 2. Define Features and Target ---\n",
        "TARGET_COL = 'Reached.on.Time_Y.N'\n",
        "\n",
        "# Features to use for the model (based on EDA findings)\n",
        "# We include Cost_of_the_Product and Customer_care_calls for completeness,\n",
        "# even though their correlation was weak, as they still contain information.\n",
        "FEATURES = df.drop(columns=[TARGET_COL]).columns\n",
        "\n",
        "X = df[FEATURES]\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "# --- 3. Identify Column Types for Preprocessing ---\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumerical Features ({len(numerical_features)}): {numerical_features}\")\n",
        "print(f\"Categorical Features ({len(categorical_features)}): {categorical_features}\")\n",
        "\n",
        "\n",
        "# --- 4. Define Preprocessing Steps (The Pipeline) ---\n",
        "\n",
        "# 4a. Numerical Pipeline: Impute Missing Values (if any) and Scale\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    # 1. Handling Nulls: Impute with the mean (Robust for most numerical data)\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    # 2. Scaling: Normalize the data to have zero mean and unit variance (StandardScaler)\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# 4b. Categorical Pipeline: Impute Missing Values (if any) and Encode\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    # 1. Handling Nulls: Impute missing categories with a constant label 'missing'\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    # 2. Encoding: One-Hot Encode (creating a new binary column for each unique category)\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# 4c. Column Transformer: Apply pipelines to the correct column sets\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep any other columns untouched (should be none here)\n",
        ")\n",
        "\n",
        "print(\"\\n--- Preprocessing Pipeline Constructed Successfully ---\")\n",
        "\n",
        "\n",
        "# --- 5. Apply Preprocessor and View Transformed Data ---\n",
        "# Note: In a real scenario, you would perform a train/test split *before* fitting the preprocessor\n",
        "# to avoid data leakage. We apply it to the whole X here for analysis visualization.\n",
        "\n",
        "# Split data to simulate real machine learning workflow (fitting only on training data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 1. FIT the preprocessor ONLY on the training data\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# 2. TRANSFORM both training and testing data\n",
        "X_train_processed = preprocessor.transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Convert the processed data back to a DataFrame for analysis/inspection\n",
        "# Get feature names after one-hot encoding\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "X_train_processed_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
        "\n",
        "\n",
        "# --- 6. Analysis and Verification ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- Analysis of Transformed Features ---\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Original Feature Count: {len(FEATURES)}\")\n",
        "print(f\"Processed Feature Count: {X_train_processed.shape[1]}\")\n",
        "print(f\"Shape of Processed Training Data: {X_train_processed.shape}\")\n",
        "\n",
        "print(\"\\nSample of Transformed Data (First 5 rows):\")\n",
        "# Show the first 5 rows of the transformed data\n",
        "print(X_train_processed_df.head())\n",
        "\n",
        "print(\"\\nVerification of Normalization (Numerical Features):\")\n",
        "# Verify that normalized features have a mean close to 0 and std dev close to 1\n",
        "normalized_check = X_train_processed_df.filter(like='num__').agg(['mean', 'std']).T.head()\n",
        "print(normalized_check)\n",
        "\n",
        "print(\"\\nVerification of Encoding (Categorical Features):\")\n",
        "# Show the columns created by One-Hot Encoding\n",
        "encoded_check = X_train_processed_df.filter(like='cat__').columns.tolist()\n",
        "print(f\"Encoded Columns Created: {encoded_check}\")\n",
        "\n",
        "print(\"\\nPreprocessing complete. Ready for model training.\")\n",
        "\n",
        "# The final processed arrays (X_train_processed, X_test_processed) are now ready for model fitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i-bhcI7ZWOe"
      },
      "source": [
        "1. Load Data\n",
        "2. Features and Split Data\n",
        "3. Column Types\n",
        "4. Robust Preprocessing Pipeline\n",
        "    * Numerical Pipeline\n",
        "    * Categorical Pipeline\n",
        "    * Column Transformer\n",
        "5. preprocessor on the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDiiU-L-cHz5"
      },
      "source": [
        "1. copy of the training data to modify step-by-step\n",
        "    * Null Values     \n",
        "    * Imputation  \n",
        "    * Normalization (Standardization)\n",
        "    * Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RpPWOJyzYylW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Load Data (Ensuring the DataFrame is named 'df') ---\n",
        "try:\n",
        "    # IMPORTANT: Use the correct file name you used previously\n",
        "    df = pd.read_csv('Train.csv')\n",
        "\n",
        "    # Drop columns identified as non-features or weak predictors\n",
        "    df = df.drop(columns=['ID', 'Customer_rating'], errors='ignore')\n",
        "\n",
        "    print(\"Data loaded successfully and non-feature columns dropped.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The data file 'your_data_file.csv' was not found. Please check the file path.\")\n",
        "    # Create mock data with a null value for demonstration\n",
        "    data = {\n",
        "        'Warehouse_block': ['A', 'B', 'C', 'A', None, 'F'],\n",
        "        'Mode_of_Shipment': ['Ship', 'Road', 'Ship', 'Flight', 'Road', 'Ship'],\n",
        "        'Customer_care_calls': [4, 5, 2, 6, 3, None], # Added a numerical null\n",
        "        'Cost_of_the_Product': [150, 200, 120, 300, 180, 250],\n",
        "        'Prior_purchases': [2, 5, 1, 8, 3, 4],\n",
        "        'Product_importance': ['low', 'medium', 'high', 'low', 'medium', 'high'],\n",
        "        'Gender': ['F', 'M', 'F', 'M', 'F', 'M'],\n",
        "        'Discount_offered': [10, 5, 25, 2, 15, 8],\n",
        "        'Weight_in_gms': [1200, 4500, 1500, 5500, 2800, 3200],\n",
        "        'Reached.on.Time_Y.N': [1, 0, 1, 0, 1, 0]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    # Introducing null values for visual demonstration\n",
        "    df.loc[4, 'Warehouse_block'] = np.nan\n",
        "    print(\"Loaded mock data with nulls for demonstration purposes.\")\n",
        "\n",
        "\n",
        "# --- 2. Define Features and Split Data ---\n",
        "TARGET_COL = 'Reached.on.Time_Y.N'\n",
        "X = df.drop(columns=[TARGET_COL])\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "# Perform train/test split (Critical to prevent data leakage)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# --- 3. Define Column Types ---\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "\n",
        "# --- 4. Define the Robust Preprocessing Pipeline ---\n",
        "\n",
        "# 4a. Numerical Pipeline: Impute Missing and Scale\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # 1. Handle Nulls by MEAN imputation\n",
        "    ('scaler', StandardScaler())                  # 2. Normalize (StandardScaler)\n",
        "])\n",
        "\n",
        "# 4b. Categorical Pipeline: Impute Missing and Encode\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # 1. Handle Nulls by 'missing' label\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # 2. Encode (One-Hot)\n",
        "])\n",
        "\n",
        "# 4c. Column Transformer: Direct data to the correct pipelines\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit the preprocessor ONLY on the training data\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# Transform both training and testing data\n",
        "X_train_processed = preprocessor.transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(\"\\n--- Preprocessing Pipeline Constructed and Applied Successfully ---\")\n",
        "\n",
        "\n",
        "# =========================================================================\n",
        "# --- VISUAL VERIFICATION: Step-by-Step Transformation Analysis ---\n",
        "# --- This section is for visual inspection only, not part of the pipeline ---\n",
        "# =========================================================================\n",
        "\n",
        "# Create a copy of the training data to modify step-by-step\n",
        "X_verify = X_train.copy()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VISUAL VERIFICATION: Handling Nulls and Normalization\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- A. Check for Null Values in Sample Data Before Imputation ---\n",
        "print(\"\\n[A] Check Null Values BEFORE Imputation (Looking for True):\")\n",
        "print(X_verify.isnull().sum())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- B. Apply Imputation (Handling Nulls) ---\n",
        "# Create Imputer Instances using the means calculated on X_train (from the pipeline fit)\n",
        "num_imputer = preprocessor.named_transformers_['num']['imputer']\n",
        "cat_imputer = preprocessor.named_transformers_['cat']['imputer']\n",
        "\n",
        "# Apply imputation\n",
        "X_verify[numerical_features] = num_imputer.transform(X_verify[numerical_features])\n",
        "X_verify[categorical_features] = cat_imputer.transform(X_verify[categorical_features])\n",
        "\n",
        "print(\"\\n[B] Check Null Values AFTER Imputation (All should be 0):\")\n",
        "print(X_verify.isnull().sum())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- C. Apply Normalization (Standardization) ---\n",
        "# Create Scaler Instance using the means/stds calculated on X_train\n",
        "scaler = preprocessor.named_transformers_['num']['scaler']\n",
        "\n",
        "# Before Scaling Check\n",
        "print(\"\\n[C.1] Numerical Means BEFORE Scaling:\")\n",
        "print(X_verify[numerical_features].mean())\n",
        "\n",
        "# Apply Scaling\n",
        "X_verify[numerical_features] = scaler.transform(X_verify[numerical_features])\n",
        "\n",
        "# After Scaling Check\n",
        "print(\"\\n[C.2] Numerical Means AFTER Scaling (Should be near 0):\")\n",
        "print(X_verify[numerical_features].mean().map('{:.10f}'.format))\n",
        "\n",
        "print(\"\\n[C.3] Sample of Normalized (Standardized) Data (Should show z-scores, not 0-1):\")\n",
        "print(X_verify[numerical_features].head(3))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- D. Apply Encoding (Optional but good for completeness) ---\n",
        "encoder = preprocessor.named_transformers_['cat']['onehot']\n",
        "X_cat_encoded = encoder.transform(X_verify[categorical_features])\n",
        "encoded_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
        "\n",
        "print(f\"\\n[D] Encoding Verification: {len(encoded_feature_names)} new columns created:\")\n",
        "print(encoded_feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw3EW9AFcy4S"
      },
      "source": [
        "Data Splitting and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RYM40NPfcl4o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "# NOTE: Replace 'your_data_file.csv' with the actual file name.\n",
        "try:\n",
        "    df = pd.read_csv('Train.csv')\n",
        "\n",
        "    # Drop columns identified as non-features or weak predictors\n",
        "    # 'ID' is dropped as an identifier. 'Customer_rating' dropped based on EDA correlation.\n",
        "    df = df.drop(columns=['ID', 'Customer_rating'], errors='ignore')\n",
        "\n",
        "    print(\"Data loaded successfully and non-feature columns dropped.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The data file 'your_data_file.csv' was not found. Please check the file path.\")\n",
        "    # Create mock data for demonstration if file is missing\n",
        "    data = {\n",
        "        'Warehouse_block': ['A', 'B', 'C', 'A', 'F'],\n",
        "        'Mode_of_Shipment': ['Ship', 'Road', 'Ship', 'Flight', 'Road'],\n",
        "        'Customer_care_calls': [4, 5, 2, 6, 3],\n",
        "        'Cost_of_the_Product': [150, 200, 120, 300, 180],\n",
        "        'Prior_purchases': [2, 5, 1, 8, 3],\n",
        "        'Product_importance': ['low', 'medium', 'high', 'low', 'medium'],\n",
        "        'Gender': ['F', 'M', 'F', 'M', 'F'],\n",
        "        'Discount_offered': [10, 5, 25, 2, 15],\n",
        "        'Weight_in_gms': [1200, 4500, 1500, 5500, 2800],\n",
        "        'Reached.on.Time_Y.N': [1, 0, 1, 0, 1]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Loaded mock data for demonstration purposes.\")\n",
        "\n",
        "\n",
        "# --- 2. Feature Engineering: Cost-to-Weight Ratio ---\n",
        "# Create the new feature: Cost_of_the_Product / Weight_in_gms\n",
        "# This represents the value density (Cost per gram)\n",
        "df['Cost_per_Gram'] = df['Cost_of_the_Product'] / df['Weight_in_gms']\n",
        "print(\"\\nNew feature 'Cost_per_Gram' engineered successfully.\")\n",
        "\n",
        "\n",
        "# --- 3. Define Features and Target ---\n",
        "TARGET_COL = 'Reached.on.Time_Y.N'\n",
        "X = df.drop(columns=[TARGET_COL])\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "print(f\"Total features now: {X.columns.tolist()}\")\n",
        "\n",
        "\n",
        "# --- 4. Split Data into Train and Test Sets ---\n",
        "# Split the data 80% Training, 20% Testing (Standard Practice)\n",
        "# stratify=y ensures the train and test sets have the same ratio of 'On-Time' (1) and 'Delayed' (0)\n",
        "# deliveries, which is crucial due to the class imbalance.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y  # Maintains the 60:40 class distribution in both sets\n",
        ")\n",
        "\n",
        "# --- 5. Analysis and Verification ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- Data Splitting and Feature Engineering Complete ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"Original Dataset Size: {df.shape[0]} rows\")\n",
        "print(f\"Training Set Size (X_train): {X_train.shape[0]} rows\")\n",
        "print(f\"Testing Set Size (X_test): {X_test.shape[0]} rows\")\n",
        "\n",
        "print(\"\\nVerification of Stratification (Target Distribution):\")\n",
        "\n",
        "# Check distribution in the original data\n",
        "original_dist = y.value_counts(normalize=True).round(4)\n",
        "print(f\"Original Distribution:\\n{original_dist}\")\n",
        "\n",
        "# Check distribution in the training data\n",
        "train_dist = y_train.value_counts(normalize=True).round(4)\n",
        "print(f\"Train Set Distribution:\\n{train_dist}\")\n",
        "\n",
        "# Check distribution in the testing data\n",
        "test_dist = y_test.value_counts(normalize=True).round(4)\n",
        "print(f\"Test Set Distribution:\\n{test_dist}\")\n",
        "\n",
        "print(\"\\n--- Feature Engineering and Train/Test Split complete. Ready for Preprocessing. ---\")\n",
        "# X_train and X_test are now the DataFrames you will use for the next step (Encoding/Normalization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlykMWKVhg39"
      },
      "source": [
        "Group By Analysis & Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9pZZ2LvcejU6"
      },
      "outputs": [],
      "source": [
        "# 5b. Group By Analysis: Delayed Performance (1 - On-Time Rate) by Category\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- Delayed Performance Rate (1 - On-Time Rate) by Category ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "categorical_cols_for_group = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']\n",
        "\n",
        "for col in categorical_cols_for_group:\n",
        "    # 1. Calculate the On-Time Rate (Mean of the binary column)\n",
        "    on_time_rate = df.groupby(col)['Reached.on.Time_Y.N'].mean()\n",
        "\n",
        "    # 2. Calculate the Delayed Rate (1 - On-Time Rate)\n",
        "    delayed_rate = 1 - on_time_rate\n",
        "\n",
        "    # Sort by the highest delayed rate\n",
        "    performance = delayed_rate.sort_values(ascending=False)\n",
        "\n",
        "    print(f\"\\nAverage Delayed Rate by {col}:\")\n",
        "    print(performance.apply(lambda x: f\"{x*100:.2f}%\")) # Display as a percentage\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Visualization of Group-by\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    performance.plot(kind='bar', color='violet') # Use 'violet' color\n",
        "    plt.title(f'Delayed Rate by {col}')\n",
        "    plt.ylabel('Delayed Percentage (1 - Mean of Reached.on.Time_Y.N)')\n",
        "    plt.xticks(rotation=0 if col == 'Gender' else 45)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S4ZriarJizwZ"
      },
      "outputs": [],
      "source": [
        "#Class Imbalance\n",
        "\n",
        "print(\"\\n--- Class Imbalance Analysis ---\")\n",
        "\n",
        "# Calculate value counts\n",
        "class_counts = df['Reached.on.Time_Y.N'].value_counts()\n",
        "\n",
        "# Calculate percentages\n",
        "class_percentages = df['Reached.on.Time_Y.N'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Create a summary DataFrame\n",
        "imbalance_df = pd.DataFrame({\n",
        "    'Count': class_counts,\n",
        "    'Percentage': class_percentages.round(2)\n",
        "})\n",
        "imbalance_df.index = ['Delayed (0)', 'On-Time (1)']\n",
        "\n",
        "print(imbalance_df)\n",
        "\n",
        "# Visualize Imbalance\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.barplot(x=imbalance_df.index, y='Count', hue=imbalance_df.index, data=imbalance_df, palette={'Delayed (0)': 'darkviolet', 'On-Time (1)': 'gray'}, legend=False)\n",
        "plt.title('Distribution of Target Classes (On-Time vs. Delayed)')\n",
        "plt.ylabel('Number of Shipments')\n",
        "plt.xlabel('Delivery Status')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzjPSmRumKAb"
      },
      "source": [
        "1. Initial Training set size\n",
        "2. Fitting preprocessor pipeline\n",
        "3. Preprocessor pipeline saved\n",
        "4. Applying SMOTE\n",
        "5. Model saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rQz5IfqYkaZy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "try:\n",
        "    # Load your initial training data file\n",
        "    df = pd.read_csv('Train.csv')\n",
        "    print(\"Train.csv loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Train.csv not found. Please ensure the file is in the current directory.\")\n",
        "    exit()\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('Reached.on.Time_Y.N', axis=1)\n",
        "y = df['Reached.on.Time_Y.N']\n",
        "\n",
        "# --- 2. Define Train/Test Split ---\n",
        "# Re-create the same split used previously\n",
        "X_train_orig, X_test, y_train_orig, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"Initial Training set size: {X_train_orig.shape}\")\n",
        "\n",
        "\n",
        "# --- 3. Define Preprocessing Pipeline ---\n",
        "\n",
        "# Identify feature types (based on your notebook structure)\n",
        "numerical_features = ['Customer_care_calls', 'Customer_rating', 'Cost_of_the_Product', 'Prior_purchases', 'Discount_offered', 'Weight_in_gms']\n",
        "categorical_features = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']\n",
        "# ID column is ignored\n",
        "\n",
        "# Create the preprocessing steps\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')), # Handle missing values (if any)\n",
        "    ('scaler', StandardScaler()) # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Handle missing values (if any)\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create the Column Transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='drop' # Drop the 'ID' column\n",
        ")\n",
        "\n",
        "# Fit the preprocessor ONLY on the original training data\n",
        "print(\"Fitting preprocessor pipeline...\")\n",
        "X_train_processed = preprocessor.fit_transform(X_train_orig)\n",
        "# We save the fitted preprocessor for Streamlit deployment\n",
        "joblib.dump(preprocessor, 'preprocessor_pipeline.joblib')\n",
        "print(\"Preprocessor pipeline saved as 'preprocessor_pipeline.joblib'.\")\n",
        "\n",
        "\n",
        "# --- 4. Apply SMOTE (Oversampling) ---\n",
        "print(\"Applying SMOTE to balance the training data...\")\n",
        "sm = SMOTE(random_state=42)\n",
        "# THESE are the variables that were missing!\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_processed, y_train_orig)\n",
        "print(f\"Resampled Training set size: {X_train_resampled.shape}\")\n",
        "\n",
        "\n",
        "# --- 5. Train and Save Final Logistic Regression Model ---\n",
        "\n",
        "# Define the best parameters (from your prior tuning results)\n",
        "best_params_logistic_regression = {\n",
        "    'penalty': 'l1',\n",
        "    'C': 0.23357\n",
        "}\n",
        "\n",
        "# Instantiate the final model\n",
        "final_logreg_model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    solver='liblinear',\n",
        "    **best_params_logistic_regression\n",
        ")\n",
        "\n",
        "# Train the model on the newly defined resampled data\n",
        "print(\"\\nTraining final Logistic Regression model...\")\n",
        "final_logreg_model.fit(X_train_resampled, y_train_resampled)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Save the model artifact\n",
        "model_filename = 'best_model_LogisticRegression.joblib'\n",
        "joblib.dump(final_logreg_model, model_filename)\n",
        "\n",
        "print(f\"\\n Success! Model successfully saved as\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDXcxK-Umb2V"
      },
      "source": [
        "Deployment Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "98RHdDBmm59Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "try:\n",
        "    df = pd.read_csv('Train.csv')\n",
        "    print(\"Train.csv loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Train.csv not found. Please ensure the file is in the current directory.\")\n",
        "    exit()\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('Reached.on.Time_Y.N', axis=1)\n",
        "y = df['Reached.on.Time_Y.N']\n",
        "\n",
        "# --- 2. Define Train/Test Split ---\n",
        "# Re-create the same split used previously\n",
        "X_train_orig, X_test, y_train_orig, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- 3. Define Preprocessing Pipeline ---\n",
        "numerical_features = ['Customer_care_calls', 'Customer_rating', 'Cost_of_the_Product', 'Prior_purchases', 'Discount_offered', 'Weight_in_gms']\n",
        "categorical_features = ['Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender']\n",
        "\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Fit the preprocessor ONLY on the original training data and SAVE it NOW\n",
        "print(\"Fitting and saving preprocessor pipeline...\")\n",
        "X_train_processed = preprocessor.fit_transform(X_train_orig)\n",
        "joblib.dump(preprocessor, 'preprocessor_pipeline.joblib')\n",
        "print(\"Preprocessor pipeline saved as 'preprocessor_pipeline.joblib'.\")\n",
        "\n",
        "\n",
        "# --- 4. Apply SMOTE (Oversampling) ---\n",
        "print(\"Applying SMOTE to balance the training data...\")\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_processed, y_train_orig)\n",
        "\n",
        "\n",
        "# --- 5. Train and Save Final Logistic Regression Model ---\n",
        "best_params_logistic_regression = {\n",
        "    'penalty': 'l1',\n",
        "    'C': 0.23357\n",
        "}\n",
        "\n",
        "final_logreg_model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    solver='liblinear',\n",
        "    **best_params_logistic_regression\n",
        ")\n",
        "\n",
        "print(\"\\nTraining final Logistic Regression model...\")\n",
        "final_logreg_model.fit(X_train_resampled, y_train_resampled)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Save the model artifact\n",
        "model_filename = 'best_model_LogisticRegression.joblib'\n",
        "joblib.dump(final_logreg_model, model_filename)\n",
        "\n",
        "print(f\"\\nfiles have been recreated with the current environment's scikit-learn version\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "634d4344"
      },
      "outputs": [],
      "source": [
        "#Class Imbalance\n",
        "\n",
        "print(\"\\n--- Class Imbalance Analysis ---\")\n",
        "\n",
        "# Calculate value counts\n",
        "class_counts = df['Reached.on.Time_Y.N'].value_counts()\n",
        "\n",
        "# Calculate percentages\n",
        "class_percentages = df['Reached.on.Time_Y.N'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Create a summary DataFrame\n",
        "imbalance_df = pd.DataFrame({\n",
        "    'Count': class_counts,\n",
        "    'Percentage': class_percentages.round(2)\n",
        "})\n",
        "imbalance_df.index = ['Delayed (0)', 'On-Time (1)']\n",
        "\n",
        "print(imbalance_df)\n",
        "\n",
        "# Visualize Imbalance\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.barplot(x=imbalance_df.index, y='Count', hue=imbalance_df.index, data=imbalance_df, palette={'Delayed (0)': 'darkviolet', 'On-Time (1)': 'gray'}, legend=False)\n",
        "plt.title('Distribution of Target Classes (On-Time vs. Delayed)')\n",
        "plt.ylabel('Number of Shipments')\n",
        "plt.xlabel('Delivery Status')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "12469e40"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure df is defined. This block makes the cell more self-sufficient.\n",
        "if 'df' not in locals() or df is None:\n",
        "    try:\n",
        "        df = pd.read_csv('Train.csv')\n",
        "        print(\"DataFrame 'df' loaded successfully within this cell.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Train.csv not found. Please upload the file and ensure the cell defining df is executed.\")\n",
        "        df = None # Ensure df is None if file not found\n",
        "\n",
        "# Manual Input Example\n",
        "\n",
        "# You can change the value of 'selected_warehouse_block' to any block (e.g., 'A', 'B', 'C', 'D', 'F')\n",
        "selected_warehouse_block = \"B\" # @param [\"A\", \"B\", \"C\", \"D\", \"F\"] {\"allow-input\":true}\n",
        "\n",
        "if df is not None:\n",
        "    print(f\"Displaying shipments from Warehouse Block: {selected_warehouse_block}\\n\")\n",
        "\n",
        "    # Filter the DataFrame based on the manual input\n",
        "    filtered_df = df[df['Warehouse_block'] == selected_warehouse_block]\n",
        "\n",
        "    # Display the first 5 rows of the filtered DataFrame\n",
        "    print(f\"Number of shipments in {selected_warehouse_block}: {len(filtered_df)}\")\n",
        "    display(filtered_df.head())\n",
        "else:\n",
        "    print(\"Cannot filter as DataFrame 'df' is not loaded or available.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl6EgO9bKWYXntW89E8b9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}